[
  {
    "objectID": "permutation_test.html",
    "href": "permutation_test.html",
    "title": "Financial Institution’s effect on GDP",
    "section": "",
    "text": "Context of the problem:\nThe point of this analysis is to explore how access to financial institutions and services such as having a checking account impacts a nations GDP and to explore the relationship between the two factors. We are exploring GDP per Capita for 120 nations in USD. We are measuring how GDP per Capita is affected by the percent of the population 15+ and older who have access to a financial institution account. Among the data we also have other variables to measure access to financial institutions such as:\nSaved at a financial institution (% age 15+)\nBorrowed from a formal financial institution (% age 15+)\nAccount, female (% age 15+)\nAccount, male (% age 15+)\nOwns a credit card (% age 15+)\nI split countries into two groups based on the median GDP per Capita:\nHigh GDP group: Countries above the median\nLow GDP group: Countries below the median\nWhy this is interesting to analyze:\nUnderstanding how economic development correlates with financial inclusion is essential for policy and development strategy. If we notice countries with greater access to financial institutions have a larger GDP per Capita than maybe we can identify financial institutions as a potential driver in generating wealth for a country. Understanding this could help addresing policy going forward in low income countries. If these countries implement policies that expand access to financial institutions it could help increase their GDP per Capita and improve a nations wealth which would also improve standard of living, overall happiness, and quality of life.\nResearch question:\nDoes greater access to financial institutions correlate with higher national wealth, as measured by GDP per Capita?\nNull hypothesis:\nThere is no difference in GDP per Capita between countries with higher and lower access to financial institutions.\nAlternate Hypothesis:\nCountries with greater access to financial institutions have a higher GDP per Capita than those with lower access.\nPlan:\nTo explore the relationship between a country’s wealth and financial access, I plan to compare the percentage of adults with bank accounts across high- and low-GDP countries. I plan to categorize countries into two groups based on the median GDP per Capita. To determine whether any observed difference in financial access was statistically significant, I plan to conduct a permutation test simulating the null hypothesis of no association. By repeatedly shuffling account access values and calculating the difference between groups, I aim to build a reference distribution to compare against the observed difference.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nGDP_analysis &lt;- read_excel(\"GDP_analysis.xlsx\", sheet = \"Data\", skip = 1)\ncolnames(GDP_analysis) &lt;- c(\n  \"Country\", \n  \"GDP_Per_Capita\", \n  \"Account_15plus\", \n  \"Saved_15plus\", \n  \"Borrowed_Formal_15plus\", \n  \"Account_Female\", \n  \"Account_Male\", \n  \"Owns_Credit_Card\")\n\nGDP_analysis &lt;- GDP_analysis |&gt; \n  mutate(across(GDP_Per_Capita:Owns_Credit_Card, as.numeric))\n  \nmedian_gdp &lt;- median(GDP_analysis$GDP_Per_Capita, na.rm = TRUE)\n\nGDP_analysis &lt;- GDP_analysis |&gt;  \n  mutate(GDP_Group = ifelse(GDP_Per_Capita &gt;= median_gdp, \"High\", \"Low\"))\n\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    select(GDP_Group, Account_15plus) |&gt;\n    mutate(Account_15plus_perm = sample(Account_15plus, replace = FALSE)) %&gt;%\n    group_by(GDP_Group) |&gt;\n    summarize(\n      obs_ave = mean(Account_15plus, na.rm = TRUE),\n      obs_med = median(Account_15plus, na.rm = TRUE),\n      perm_ave = mean(Account_15plus_perm, na.rm = TRUE),\n      perm_med = median(Account_15plus_perm, na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt;\n    summarize(\n      obs_ave_diff = diff(obs_ave),\n      obs_med_diff = diff(obs_med),\n      perm_ave_diff = diff(perm_ave),\n      perm_med_diff = diff(perm_med),\n      rep = rep\n    )\n}\n\nperm_results &lt;- map_df(1:1000, perm_data, data = GDP_analysis)\n\nhead(perm_results)\n\n# A tibble: 6 × 5\n  obs_ave_diff obs_med_diff perm_ave_diff perm_med_diff   rep\n         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1       -0.459       -0.572      -0.103         -0.204      1\n2       -0.459       -0.572       0.113          0.277      2\n3       -0.459       -0.572       0.138          0.302      3\n4       -0.459       -0.572      -0.00166       -0.0220     4\n5       -0.459       -0.572      -0.0940        -0.214      5\n6       -0.459       -0.572      -0.0350        -0.114      6\n\n\nMy Analysis:\nIn my analysis I aimed to investigates whether countries with higher GDP per Capita also have greater access to financial institutions. To do this, I began by importing and cleaning the dataset, converting relevant financial indicators to numeric form. I then split countries into “High” and “Low” GDP groups based on the median GDP per Capita. The main variable of interest is the percentage of adults aged 15+ with a bank account. To test whether the observed difference in account access between high- and low-GDP countries could be due to chance, I implemented a permutation test. This involved randomly shuffling the account access values across countries 1,000 times to simulate the distribution of differences we would expect under the null hypothesis of no relationship. For each iteration, I calculated the observed and permuted differences in both means and medians between the two groups. The results allow us to compare the actual observed difference to what we’d expect under random assignment, helping determine whether the relationship between GDP and financial access is statistically meaningful.\n\nlibrary(ggplot2)\n\nGDP_analysis |&gt;\n  ggplot(aes(x = GDP_Group, y = Account_15plus, fill = GDP_Group)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(\n    title = \"Access to Financial Accounts by GDP Group\",\n    x = \"GDP Group\",\n    y = \"% of Population (15+) with an Account\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Low\" = \"#FF9999\", \"High\" = \"#66CC99\"))\n\n\n\n\n\n\n\n\nThis plot shows a comparison of access to financial accounts between countries with high and low GDP per Capita. Each box represents the distribution of the percentage of adults aged 15 and older who have an account at a financial institution within each GDP group. It shows the lower 25th percent quartile represented by the bottom of the box. This middle of the box represents the 50th percent quartile. The top of the box represents the 75th percent quartile. It also shows the outliers and data not in this distribution by the lines and data points shown outside of the box. From the visualization, we see that countries with higher GDP per Capita generally have greater access to financial accounts as we see the box is a lot higher. This means that on average those in higher GDP per Capita countries have a greater percent of the population with access to financial accounts. We see both the median and overall distribution higher than in the low-GDP group. This visual difference supports the idea that national wealth may be associated with financial inclusion."
  },
  {
    "objectID": "DataEthics.html",
    "href": "DataEthics.html",
    "title": "DataEthics",
    "section": "",
    "text": "Introduction:\nImagine you get caught committing a petty crime, such as stealing someone’s purse out of their car, and you find yourself in court negotiating your parole and sentence with a judge. During this delegation, you are asked to fill out a test of 137 questions to predict how dangerous you may be in the future and whether you will get parole. Now, imagine you are a person of color: you may find that this algorithm predicted you to be much more dangerous in the future than a white individual who did something similar or worse, and you find yourself with a harsher sentence than that white individual. Also, imagine you learn information about previous offenses and data on you is being fed to this algorithm without your consent—an algorithm designed to determine how to rehabilitate people, not how to sentence them.\nThis happens quite often with the use of a predictive algorithm called COMPAS, which analyzes answers to 137 questions, ranging from criminal history to personal background (like whether your parents were incarcerated or if you have a job), to assess your risk level. This proprietary tool is designed to forecast the likelihood of a criminal defendant reoffending and is used in several stages of the U.S. criminal justice system, including pretrial release decisions and sentencing. COMPAS generates risk scores by analyzing answers to 137 items derived from both personal interviews and criminal records. \nThe ProPublica analysis, which was used in the article to evaluate whether the tool held racial bias, involved collecting real-world risk scores from over 7,000 individuals arrested in Broward County, Florida. They then checked their actual criminal records over the following two years. They used this data to statistically evaluate the accuracy and fairness of the COMPAS predictions. While the tool was about 61% accurate overall (slightly better than random guessing), ProPublica found that it made systematic errors based on race: it overestimated the risk for Black defendants and underestimated it for white defendants.\nThis poses an ethical dilemma in that this algorithm has been found to produce biased results. ProPublica’s investigation showed that Black defendants were far more likely to be labeled high risk even when they did not reoffend, while white defendants with worse records were often labeled low risk. These scores can and have deeply influenced people’s freedom, yet the individuals affected don’t know how the scores are calculated and have limited ability to challenge them. This raises serious concerns about transparency, fairness, and racial bias in automated decision-making systems used by the justice system.\nWhat was the consent structure for recruiting participants? Were the participants aware of the ways their data would be used for research? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen?\nThe core issue here is informed consent, which requires that participants understand how their data will be used and voluntarily agree to those uses. In the case of COMPAS, individuals were not recruited as research participants, but they were criminal defendants whose data was collected to determine the proceedings of their court cases. As highlighted in the ProPublica investigation, these defendants were likely unaware that their answers to personal questions, such as if a parent had been to prison, would later be used in said proceedings as well as journalism and academic research. The WIRED article by Lapowsky supports this concern by showing how these data, originally collected for case processing, were later used to evaluate algorithmic fairness and predictive accuracy, often without any consent from the individuals involved. This raises serious questions about whether informed consent is even possible in a setting like the criminal justice system. More broadly, both articles illustrate how difficult it is to secure meaningful consent for future, and other unforeseen uses, especially when those uses include third-party audits, news reports, or machine learning research.\nIs the data being used in unintended ways to the original study?\nThe core issue here is mitigating bias in ourselves and the data we apply to the real world, especially when we are applying it to the freedom of others. This involves identifying and correcting disparities in how predictive models treat different demographic groups. The ProPublica article exposed how COMPAS produced racially skewed predictions: Black defendants were far more likely than white defendants to be falsely labeled as high-risk. Even though the model did not explicitly use race, it incorporated proxies, such as education, employment, and family history, that have underlying systemic racial inequalities. The WIRED article by Lapowsky extends this point, showing how Dartmouth researchers compared COMPAS to predictions made by Mechanical Turk workers and found similar racial disparities in both cases, despite omitting race as a data point. This suggests that these unintended biases are an underlying problem in the algorithm. The fact that COMPAS was used beyond its original scope which is influencing sentencing decisions without reevaluation or transparency, further shows the unintended uses of data which continue to exacerbate structural inequalities. \nShould race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender?\nThe core issue of this question is to consider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society. It’s important to recognize if race should be used as a variable in this study or if it is used as a proxy for something else. According to ProPublica, COMPAS does not use race directly, but the model’s reliance on variables like prior convictions, family incarceration, and employment effectively makes race a proxy. The WIRED article confirms that even when race was withheld from human participants and machine learning models, the false positive rate remained higher for Black defendants, suggesting that race is indirectly encoded through other features. This shows that omitting race doesn’t remove bias but makes it less visible. The Dartmouth study also revealed that only two variables age and number of prior offenses were needed to match COMPAS’s accuracy, but these too reflect racial disparities in policing and prosecution. Gender operates similarly: while it may appear neutral, its inclusion can reinforce stereotypes and institutional inequities unless handled with care. Together, these articles argue that merely excluding race or gender does not ensure fairness but in fact, it may prevent the detection and correction of embedded bias. An interesting thought could be to add race into these models to in a sense dilute the consideration of other variables such as family incarceration and employment. Systematic inequalities make these attributes more common in Black Americans and if we can recognize that in the model we could weigh those variables less harshly. However, this could backfire and make the model more racist.\nHow were the variables collected? Were they accurately recorded? Is there any missing data?\nThe core issue here is data integrity, ensuring that the variables used to power predictive algorithms are accurate, complete, and reliable. In ProPublica’s investigation, the COMPAS tool drew from self-reported questionnaires and criminal records, including subjective questions like one’s attitudes toward authority or peer behavior. This raises questions about the validity of responses gathered especially in an institutional setting. The WIRED article echoes these concerns by pointing out that COMPAS amasses up to 137 data points per defendant, many of which are unverifiable or not publicly documented. Furthermore, Equivant, the company behind COMPAS, has never disclosed the full list of variables or their weights, making external auditing difficult. This opacity prevents researchers from checking for missing data, measurement error, or inconsistency in data collection methods. According to the WIRED piece, even when researchers built a simplified model with just seven variables, it performed similarly to COMPAS suggesting that the extra data may not add meaningful accuracy and could introduce more bias or error. Without transparency, there is no way to ensure that the model’s decisions are based on solid evidence rather than flawed inputs.\nCitations:\nAngwin, Julia, et al. “Machine Bias: There’s Software Used across the Country to Predict Future Criminals. And It’s Biased against Blacks.” ProPublica, 23 May 2016, www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\nLapowsky, Issie. “Crime-Predicting Algorithms May Not Fare Much Better Than Untrained Humans.” WIRED, 17 Jan. 2018, https://www.wired.com/story/crime-predicting-algorithms-may-not-outperform-untrained-humans/."
  },
  {
    "objectID": "NFLsalaries.html",
    "href": "NFLsalaries.html",
    "title": "NFL Salaries by Position",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nfile_path &lt;- \"nfl_salary.xlsx\"\nnfl_salary &lt;- read_excel(file_path)\n\nggplot(nfl_salary, aes(x = Quarterback)) +\n  geom_boxplot(color = \"navy\")+\n  facet_wrap(~year)+\n  labs(title = \"Average Quarterback salaries throughout the years\")"
  },
  {
    "objectID": "FastFood.html",
    "href": "FastFood.html",
    "title": "FastFood",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\nfile_path &lt;- \"Fast Food.xlsx\"\nFast_Food &lt;- read_excel(file_path)\n\nggplot(Fast_Food, aes(x = calories)) +\n  geom_boxplot(color = \"darkred\")+\n  facet_wrap(~restaurant)+\n  labs(title = \"Average calories per restaurant\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nathan Dhanani",
    "section": "",
    "text": "Hello! My name is Nathan Dhanani and I am an Economic/Finance student at Claremont McKenna College. I am hoping to be an Economic Consultant and am currently interning at Analysis Group. I love playing soccer, lifting weights, pickle ball and Spikeball. Feel free to browse my website to learn more."
  },
  {
    "objectID": "Stinganalysis.html",
    "href": "Stinganalysis.html",
    "title": "StringAnalysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(RTextTools)\ndata(NYTimes)\nas_tibble(NYTimes)\n\n# A tibble: 3,104 × 5\n   Article_ID Date      Title                                 Subject Topic.Code\n        &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                                 &lt;fct&gt;        &lt;int&gt;\n 1      41246 1-Jan-96  Nation's Smaller Jails Struggle To C… Jails …         12\n 2      41257 2-Jan-96  FEDERAL IMPASSE SADDLING STATES WITH… Federa…         20\n 3      41268 3-Jan-96  Long, Costly Prelude Does Little To … Conten…         20\n 4      41279 4-Jan-96  Top Leader of the Bosnian Serbs Now … Bosnia…         19\n 5      41290 5-Jan-96  BATTLE OVER THE BUDGET: THE OVERVIEW… Battle…          1\n 6      41302 7-Jan-96  South African Democracy Stumbles on … politi…         19\n 7      41314 8-Jan-96  Among Economists, Little Fear on Def… econom…          1\n 8      41333 10-Jan-96 BATTLE OVER THE BUDGET: THE OVERVIEW… budget…          1\n 9      41344 11-Jan-96 High Court Is Cool To Census Change   census…         20\n10      41355 12-Jan-96 TURMOIL AT BARNEYS: THE DIFFICULTIES… barney…         15\n# ℹ 3,094 more rows\n\n\n\nNYTimesv2 &lt;- NYTimes |&gt;\n  mutate(politics_mention = str_detect(Title,regex(\"election|vote|senate|president|congress|politics|congress|clinton|trump\", ignore_case = TRUE))) |&gt;\n  mutate(title_word_count = str_count(Title,\"\\\\S+\")) |&gt;\n  mutate(location = str_extract(Title, regex(\"New York|California|Texas|France|Russia|China|Japan|Mexico|Bosnia|Serbia|Germany|UK|USA\", ignore_case = TRUE)))|&gt;\n  mutate(location = str_to_lower(location)) |&gt;\n  filter(politics_mention == TRUE, !is.na(location)) |&gt;\n  group_by(location) |&gt;\n  summarize(article_count = n(), .groups = \"drop\")\n\n\nggplot(NYTimesv2, aes(x = reorder(location, -article_count), y = article_count, fill = location)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Most Mentioned Locations in Political Articles\",\n       x = \"Location\",\n       y = \"Number of Articles\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot displays the frequency of political articles mentioning specific locations. Each bar represents a location, with the height indicating the number of articles referencing it. Locations are ordered from most to least mentioned, providing a clear view of geographic trends in political news coverage.\n\nlibrary(lubridate)\nNYTimesv3 &lt;- NYTimes |&gt;\n  mutate(Date = dmy(Date),  \n         Year = year(Date),\n         politics_mention = str_detect(Title, regex(\"election|vote|senate|president|congress|politics|congress|clinton|trump\", ignore_case = TRUE)),\n         title_word_count = str_count(Title,\"\\\\S+\"),\n         location = str_extract(Title, regex(\"New York|California|Texas|France|Russia|China|Japan|Mexico|Bosnia|Serbia|Germany|UK|USA\", ignore_case = TRUE))) |&gt;\n  filter(politics_mention == TRUE, !is.na(location)) |&gt;\n  group_by(Year) |&gt;\n  summarise(article_count = n(), .groups = \"drop\") |&gt;\n  arrange(Year)\n\n\nggplot(NYTimesv3, aes(x = Year, y = article_count)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(size = 2, color = \"red\") +\n  labs(title = \"Political Articles Mentioning Key Locations Over Time\",\n       x = \"Year\",\n       y = \"Number of Articles\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis line graph illustrates the number of political articles referencing key locations across different years. The x-axis represents the year, while the y-axis shows the total number of articles that mention political topics alongside specific locations. The blue line indicates trends over time, while red points highlight the exact number of articles per year. This visualization helps identify fluctuations in political coverage across the dataset."
  }
]