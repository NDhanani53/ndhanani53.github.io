[
  {
    "objectID": "permutation_test.html",
    "href": "permutation_test.html",
    "title": "Financial Institution’s effect on GDP",
    "section": "",
    "text": "Context of the problem:\nThe point of this analysis is to explore how access to financial institutions and services such as having a checking account correlates with a nation’s GDP and to explore the relationship between the two. We are exploring GDP per capita for 120 nations in USD. We are measuring how GDP per capita is correlates with the percent of the population 15+ and older who have access to a financial institution account. Among the data we also have other variables to measure access to financial institutions such as:\nSaved at a financial institution (% age 15+)\nBorrowed from a formal financial institution (% age 15+)\nAccount, female (% age 15+)\nAccount, male (% age 15+)\nOwns a credit card (% age 15+)\nI split countries into two groups based on the median GDP per capita:\nHigh GDP group: Countries above the median\nLow GDP group: Countries below the median\nWhy this is interesting to analyze:\nUnderstanding if economic development relates to financial inclusion can be interesting to study to hopefully understand what makes a nation wealthy and may aid in policy and development strategy. If we notice countries with greater access to financial institutions have a larger GDP per capita than maybe we can identify financial institutions as one of many potential drivers in generating wealth for a country. Understanding this could help address policy going forward in low-income countries. If these countries implement policies that expand access to financial institutions, it could help increase their GDP per capita and improve a nation’s wealth, which would also improve the standard of living, overall happiness, and quality of life. Though studying this relationship does not mean there is a direct cause, it could be the start of a study in which we could hopefully identify the cause eventually. We are mostly understanding the relationship, but if there is a significant relationship, it is interesting to hypothesize that there may be some causation.\nExploratory Analysis:\n\n# Load and clean GDP dataset, assign GDP groups based on the median GDP per capita\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nGDP_analysis &lt;- read_excel(\"GDP_analysis.xlsx\", sheet = \"Data\", skip = 1)\ncolnames(GDP_analysis) &lt;- c(\n  \"Country\", \n  \"GDP_Per_Capita\", \n  \"Account_15plus\", \n  \"Saved_15plus\", \n  \"Borrowed_Formal_15plus\", \n  \"Account_Female\", \n  \"Account_Male\", \n  \"Owns_Credit_Card\")\n\nDP_analysis &lt;- GDP_analysis |&gt; \n  mutate(across(GDP_Per_Capita:Owns_Credit_Card, as.numeric))\n  \nmedian_gdp &lt;- GDP_analysis |&gt;\n  summarise(median_gdp = median(GDP_Per_Capita, na.rm = TRUE)) |&gt;\n  pull(median_gdp)\n\nGDP_analysis &lt;- GDP_analysis |&gt;  \n  mutate(GDP_Group = ifelse(GDP_Per_Capita &gt;= median_gdp, \"High\", \"Low\"))\n\n\n#| fig-alt: \"This chart shows two boxes, one for high-GDP countries and one for low-GDP countries, to compare the percentage of people age 15 and older who have a financial account. The x-axis shows the GDP group (high or low), and the y-axis shows the percentage of account ownership. Each box shows the range of values within that group. The line in the center of each box marks the median, or typical value. High-GDP countries have a higher median, meaning account access is more common and consistent. Low-GDP countries have lower percentages.\"\n\n# Creates a boxplot to compare account ownership (age 15+) between high and low-GDP countries.\n\nGDP_analysis |&gt;\n  ggplot(aes(x = GDP_Group, y = Account_15plus)) +\n  geom_boxplot() + \n  labs(\n    title = \"Financial Account Ownership by GDP Group\",\n    x = \"GDP Group (High vs Low Income Countries)\",\n    y = \"Percent with Financial Account (Age 15+)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nResearch question:\nDoes greater access to financial institutions correlate with higher national wealth, as measured by GDP per capita?\nNull hypothesis:\nThere is no difference in the degree of access to financial institutions between countries with higher and lower GDP per capita.\nAlternate Hypothesis:\nCountries with a higher GDP per capita have greater access to financial institutions than those with a lower GDP per capita.\nPlan:\nTo explore the relationship between a country’s wealth and financial access, I plan to compare the percentage of adults with bank accounts across high- and low-GDP countries. I plan to categorize countries into two groups based on the median GDP per capita. To determine whether any observed difference in financial access was statistically significant, I plan to conduct a permutation test simulating the null hypothesis of no association. By repeatedly shuffling account access values and calculating the difference between groups, I aim to build a reference distribution to compare against the observed difference.\n\n# Define a function to generate one permutation sample and calculate differences in means and medians\n\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    select(GDP_Group, Account_15plus) |&gt;\n\n# Randomly permute account ownership values (simulate the null hypothesis)\n\n    mutate(Account_15plus_perm = sample(Account_15plus, replace = FALSE)) |&gt;\n    \n# Group by GDP group to compute observed and permuted group summaries\n\n    group_by(GDP_Group) |&gt;\n    summarize(\n      obs_ave = mean(Account_15plus, na.rm = TRUE),\n      obs_med = median(Account_15plus, na.rm = TRUE),\n      perm_ave = mean(Account_15plus_perm, na.rm = TRUE),\n      perm_med = median(Account_15plus_perm, na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt;\n    \n# Calculate the differences between groups (Low GDP - High GDP)\n\n    summarize(\n      obs_ave_diff = diff(obs_ave),\n      obs_med_diff = diff(obs_med),\n      perm_ave_diff = diff(perm_ave),\n      perm_med_diff = diff(perm_med),\n      rep = rep\n    )\n}\n\nperm_results &lt;- map_df(1:1000, perm_data, data = GDP_analysis)\n\nhead(perm_results)\n\n# A tibble: 6 × 5\n  obs_ave_diff obs_med_diff perm_ave_diff perm_med_diff   rep\n         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1       -0.459       -0.572       0.0185         0.0934     1\n2       -0.459       -0.572      -0.0109        -0.114      2\n3       -0.459       -0.572       0.00156       -0.0881     3\n4       -0.459       -0.572       0.0648         0.0819     4\n5       -0.459       -0.572      -0.0925        -0.276      5\n6       -0.459       -0.572       0.0558         0.151      6\n\n\nMy Analysis:\nIn my analysis, I aimed to investigate whether countries with higher GDP per capita also have greater access to financial institutions. To do this, I began by importing and cleaning the dataset, converting relevant financial indicators to numeric form. I then split countries into “High” and “Low” GDP groups based on the median GDP per capita. The main variable of interest is the percentage of adults aged 15+ with a bank account. To test whether the observed difference in account access between high- and low-GDP countries could be due to chance, I implemented a permutation test. This involved randomly shuffling the account access values across countries 1,000 times to simulate the distribution of differences we would expect under the null hypothesis of no relationship. For each iteration, I calculated the observed and permuted differences in both means and medians between the two groups. The results allow us to compare the actual observed difference to what we’d expect under random assignment, helping determine whether the relationship between GDP and financial access is statistically meaningful.\n\n#| fig-alt: \"This chart shows a histogram, which is a series of bars representing how often certain differences in means appeared during a permutation test. The x-axis shows the difference in account ownership between low- and high-GDP countries, and the y-axis shows how many times each difference occurred across the randomized trials. The red vertical line marks the actual observed difference in the real data. This red line is far from the center of the bars and very much to the left. This suggests the real result is unusual and may be statistically significant.\"\n\n# This chunk plots the  distribution of mean differences in account ownership between GDP groups that were ran in the simulation\n\n\nggplot(perm_results, aes(x = perm_ave_diff)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(obs_ave_diff)), color = \"red\") +\n  labs(\n    title = \"Permutation Test: Difference in Means of Account Ownership (15+)\",\n    x = \"Difference in Means (Low GDP - High GDP)\",\n    y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n#| fig-alt: \"This chart shows a histogram, which is a series of bars representing how often certain differences in medians appeared during a permutation test. The x-axis shows the difference in account ownership between low- and high-GDP countries, and the y-axis shows how many times each difference occurred across the randomized trials. The blue vertical line marks the actual observed difference in the real data. This red line is far from the center of the bars and very much to the left. This suggests the real result is unusual and may be statistically significant.\"\n\n# This chunk plots the  distribution of median differences in account ownership between GDP groups that were ran in the simulation\n\n\nggplot(perm_results, aes(x = perm_med_diff)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(obs_med_diff)), color = \"blue\",) +\n  labs(\n    title = \"Permutation Test: Difference in Medians of Account Ownership (15+)\",\n    x = \"Difference in Medians (Low GDP - High GDP)\",\n    y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBoth plots present the results of a permutation test designed to evaluate whether there is a statistically significant difference in account ownership (among individuals aged 15 and older) between low-GDP and high-GDP countries. The x-axis on the first plot shows the possible differences in mean account ownership between the two GDP groups that were generated by randomly shuffling the group labels (i.e., permuting which countries are labeled “low” or “high” GDP). In the second plot, it presents the same thing but with the median. Each bar in the histogram represents the frequency of a particular difference in means/medians arising purely by chance under the null hypothesis that GDP classification does not affect financial account ownership.\nThe purpose of this test is to create a distribution of outcomes we might expect to see if there were no real relationship between GDP level and account access, essentially simulating the behavior of the data under random assignment. The vertical line on the graph marks the observed difference in means from the actual dataset (without permutation). This line serves as a reference point: its position relative to the bulk of the histogram tells us whether the observed difference is extreme or typical compared to what might occur under random chance.\nThe lines lie far out to the left where not even one simulation occurs, This indicates that the observed difference did not occur in the 1000 simulations, meaning it is unlikely to occur by chance. This rejects the null hypothesis and suggests that GDP level may be associated with account ownership. Though, since there may be many other factors at play, we can’t claim any sort of causation.\nCitation:\nDemirgüç-Kunt, A., Klapper, L., Singer, D., & Ansar, S. (2022). The Global Findex Database 2021: Financial Inclusion, Digital Payments, and Resilience in the Age of COVID-19. The World Bank. https://www.worldbank.org/en/publication/globalfindex/Data\nWho collected the data? The World Bank’s Development Research Group.\nWhy? To measure how adults worldwide access and use financial services, including accounts, payments, savings, credit, and financial resilience.\nOriginal source: The Global Findex Database 2021, based on nationally representative surveys conducted in over 140 economies.\nWorldometer. (2024). GDP per capita. Retrieved from https://www.worldometers.info/gdp/gdp-per-capita/\nWho collected the data? Worldometer, compiling data from sources such as the World Bank’s World Development Indicators.\nWhy? To provide up-to-date GDP per capita figures for countries worldwide, facilitating economic comparisons.\nOriginal source: World Bank’s World Development Indicators, as cited by Worldometer.\nI combind two different data sets that came from these cited sources and complied it all into one data set"
  },
  {
    "objectID": "DataEthics.html",
    "href": "DataEthics.html",
    "title": "DataEthics",
    "section": "",
    "text": "Introduction:\nImagine you get caught committing a petty crime, such as stealing someone’s purse out of their car, and you find yourself in court negotiating your parole and sentence with a judge. During this delegation, you are asked to fill out a test of 137 questions to predict how dangerous you may be in the future and whether you will get parole. Now, imagine you are a person of color. You may find that the algorithm predicted you to be much more dangerous in the future than a white individual who did something similar or worse, and you receive a harsher sentence. Also, imagine you learn that information about your previous offenses and personal history was fed to the algorithm without your consent. The algorithm was designed to determine how to rehabilitate people, not how to sentence them.\nThis situation often happens with the use of a predictive algorithm called COMPAS, which analyzes answers to 137 questions, ranging from criminal history to personal background, such as whether your parents were incarcerated or whether you have a job, to assess your risk level. The tool is designed to forecast the likelihood of a criminal defendant reoffending and is used at several stages of the U.S. criminal justice system, including pretrial release decisions and sentencing. COMPAS generates risk scores by analyzing data collected from interviews and criminal records.\nJulia Angwin and her colleagues at ProPublica conducted an investigation into COMPAS using data they obtained from the Broward County Sheriff’s Office through the Freedom of Information Act. They collected real-world COMPAS risk scores for over 7,000 individuals arrested in Broward County, Florida, and compared them with actual criminal records over the following two years. This data was used to statistically evaluate the accuracy and fairness of COMPAS predictions. Although the algorithm was about 61% accurate overall, slightly better than random guessing, Angwin’s team found that it made systematic errors based on race. The scores often overestimated risk for Black defendants and underestimated it for white defendants.\nAngwin’s analysis raises serious ethical concerns. Black defendants were far more likely to be labeled high-risk even when they did not reoffend, while white defendants with worse records were often labeled low-risk. These scores can and have influenced people’s freedom, yet the individuals affected do not know how the scores are calculated and have limited ability to challenge them. This lack of transparency and accountability raises urgent concerns about fairness and racial bias in automated decision-making systems used by the justice system.\nWhat was the consent structure for recruiting participants? Were the participants aware of the ways their data would be used for research? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen?\nThe core issue here is informed consent, which requires that participants understand how their data will be used and voluntarily agree to those uses. In the case of COMPAS, individuals were not recruited as research participants. They were criminal defendants whose data was collected to determine the proceedings of their court cases. As Angwin’s investigation shows, defendants were likely unaware that their answers to personal questions, such as whether a parent had been to prison, would later be used in both court proceedings and in external journalism and academic research. Issie Lapowsky, in her WIRED article, reinforces this concern by explaining how data originally collected for case processing was later used to evaluate algorithmic fairness and predictive accuracy, often without any informed consent from the individuals involved. This raises serious questions about whether meaningful consent is even possible in a setting like the criminal justice system. Both Angwin and Lapowsky show how difficult it is to secure valid consent for future and unforeseen uses, especially when those uses include third-party audits, journalistic investigations, or machine learning research.\nIs the data being used in unintended ways to the original study?\nThe main issue here is mitigating bias in both our models and ourselves, especially when we apply predictive tools to situations that directly affect people’s freedom. This involves identifying and correcting disparities in how predictive models treat different demographic groups. Angwin’s investigation showed that COMPAS produced racially skewed predictions. Black defendants were far more likely than white defendants to be falsely labeled as high-risk. Although COMPAS does not include race as a direct input, the model incorporates proxy variables, such as education level, employment status, and family history, that are deeply shaped by systemic racial inequalities. Lapowsky expands on this point by describing how Dartmouth researchers compared COMPAS to predictions made by Mechanical Turk workers and found similar racial disparities, even though neither group used race as a variable. This suggests that bias is embedded in the structure of the data itself. The fact that COMPAS was used in sentencing decisions, even though it was originally intended for case management, shows how predictive tools can be used in unintended ways, increasing the risk of harm.\nShould race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender?\nThe core question here is whether including or excluding race makes a model more fair or more biased. According to Angwin, COMPAS does not directly include race as a variable. However, variables like prior convictions, family incarceration, and employment status serve as proxies. These are not neutral features; they are shaped by long-standing inequalities in housing, education, and policing. Lapowsky’s article confirms that even when race was withheld from both humans and algorithms, false positive rates were still higher for Black defendants. This shows that excluding race does not eliminate bias but instead makes it harder to detect and correct. Dartmouth researchers found that just two variables—age and number of prior offenses—were enough to replicate COMPAS’s predictions, but these also reflect racial disparities in policing and prosecution. Gender works in a similar way. While it may appear to be a neutral input, it can reinforce stereotypes and institutional biases if not handled carefully. Both Angwin and Lapowsky argue that removing sensitive attributes like race or gender from predictive models does not ensure fairness and may actually hide inequities. One possibility would be to explicitly include race and use it to weigh certain variables, such as family incarceration and employment status, less heavily for historically marginalized groups. However, this strategy could backfire and introduce other forms of unfairness if not applied carefully.\nHow were the variables collected? Were they accurately recorded? Is there any missing data?\nThe key concern here is data integrity. In Angwin’s investigation, COMPAS relied on both self-reported questionnaires and criminal records. Many of the 137 variables are based on personal opinions or unverifiable information, such as a person’s views on authority or their peers’ behavior. This raises questions about accuracy, especially in institutional settings where individuals may not respond freely or truthfully. Lapowsky also notes that COMPAS collects dozens of data points per defendant, many of which are opaque or not publicly disclosed. The company behind COMPAS has never published the full list of variables or how they are weighted, making it impossible to audit the system. This lack of transparency means researchers and affected individuals cannot check for missing data or inconsistent methods. According to Lapowsky, even when researchers used only seven variables in a simplified model, they achieved similar accuracy to COMPAS. This suggests that much of the additional data may not improve performance and could introduce more bias or error.\nCitations:\nAngwin, Julia, et al. “Machine Bias: There’s Software Used across the Country to Predict Future Criminals. And It’s Biased against Blacks.” ProPublica, 23 May 2016, www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\nLapowsky, Issie. “Crime-Predicting Algorithms May Not Fare Much Better Than Untrained Humans.” WIRED, 17 Jan. 2018, https://www.wired.com/story/crime-predicting-algorithms-may-not-outperform-untrained-humans/."
  },
  {
    "objectID": "NFLsalaries.html",
    "href": "NFLsalaries.html",
    "title": "NFL Salaries by Position",
    "section": "",
    "text": "Introduction:\nTo explore how player compensation varies across different roles in the NFL, I analyzed a data set containing salary data by position. Initially, the data was in wide format, with each column representing a different player position. To better visualize and compare the salary distributions across these positions, I reshaped the data set into a long format using a pivot operation. This allowed me to create a single box plot chart displaying the spread, median, and variability of salaries for each position. The resulting visualization reveals clear disparities being: the consistently higher salaries earned by quarterbacks, while positions like special teams and safeties tend to fall on the lower end of the pay scale. This plot offers an accessible summary of the economic hierarchy within professional football.\nExploratory Analysis:\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Load data\nnfl_salary &lt;- read_excel(\"nfl_salary.xlsx\")\n\n# Reshape data from wide to long format\nnfl_long &lt;- nfl_salary |&gt;\n  pivot_longer(cols = -year, names_to = \"position\", values_to = \"salary\")\n\n# Summarize total salary by position (across all years)\nnfl_summary &lt;- nfl_long |&gt;\n  group_by(position) |&gt;\n  summarise(total_salary = sum(salary, na.rm = TRUE)) |&gt;\n  arrange(desc(total_salary))\n\n# Create bar chart\nggplot(nfl_summary, aes(x = reorder(position, total_salary), y = total_salary)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Total NFL Salary by Position (All Years Combined)\",\n    x = \"Position\",\n    y = \"Total Salary\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis bar chart shows the total salary earned by each NFL position across all years included in the data set. It provides a high-level overview of the data by highlighting which positions command the most salary overall. For example, positions like Defensive and Offensive Lineman clearly stand out as top earners, while roles like Special Teamer (special team positions) and Tight End earn less in aggregate. This chart helps the reader understand the distribution of financial investment across positions and serves as a starting point before diving into deeper analyses of salary variation or trends.\nVariables:\n\nyear - The year the salary data was recorded\nCorner back - Salary of the top-paid cornerbacks that year (USD)\nDefensive Lineman - Salary of the top-paid defensive linemen (USD)\nLinebacker - Salary of the top-paid linebackers (USD)\nOffensive Lineman - Salary of the top-paid offensive linemen (USD)\nQuarterback - Salary of the top-paid quarterbacks (USD)\nRunning Back - Salary of the top-paid running backs (USD)\nSafety - Salary of the top-paid safeties (USD)\nSpecial Teamer - Salary of the top-paid special teams players (USD)\nTight End - Salary of the top-paid tight ends (USD)\nWide Receiver - Salary of the top-paid wide receivers (USD)\n\n\n#| fig-alt: \"This chart compares NFL salaries by position. Each box shows the range of salaries for a position across all years, with the line inside representing the median. Taller or higher boxes indicate higher or more variable salaries. Quarterbacks and Offensive Linemen tend to earn the most, while positions like Special Teamers and Tight Ends earn less and show less variation in pay.\"\n#| \n# Reshape the data from wide to long format\n# Keeps 'year' as is, and stacks the salary data by position into two columns: 'position' and 'salary'\n\nnfl_long &lt;- nfl_salary |&gt;\n  pivot_longer(\n    cols = -year,\n    names_to = \"position\",\n    values_to = \"salary\"\n  )\n\n# Create a boxplot to visualize salary distribution by NFL position\n\nggplot(nfl_long, aes(x = position, y = salary)) +\n  geom_boxplot(fill = \"lightblue\") +\n  theme_minimal() +\n  labs(title = \"NFL Salaries by Position\", x = \"Position\", y = \"Salary\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThis box plot shows the distribution of NFL player salaries by position. Each box represents the range of salaries for a given position across multiple players, highlighting the median, 25th % and 75th percentile, and potential outliers. It allows for easy comparison of pay differences between positions, such as the typically higher salaries of quarterbacks compared to special team positions.\nCitations:\nTidyTuesday source: Mock, T., & TidyTuesday Project. (2018, April 9). NFL salary data. TidyTuesday. Retrieved from https://github.com/rfordatascience/tidytuesday/blob/main/data/2018/2018-04-09/nfl_salary.xlsx\n\nWho collected the data: The data set was compiled and shared by Thomas Mock as part of the weekly TidyTuesday project.\nWhat is the original source of the information: The salary data was scraped from Spotrac, a site that tracks sports contracts and salaries.\nWhy was the data collected: This data set was curated to encourage open data analysis, reproducible research, and collaborative learning among the R community.\n\nOriginal data source: Original data from Spotrac. (n.d.). NFL player contracts. Spotrac. Retrieved from https://www.spotrac.com/nfl/rankings/\n\nWho collected the data: The data was collected and published by Spotrac, a private sports analytics and contract tracking company. Spotrac compiles, maintains, and publicly shares detailed financial data on professional athletes across major sports leagues, including the NFL.\nWhat is the original source of the information: The original source of the data is a combination of publicly available contract disclosures from NFL teams, filings from the NFL Players Association (NFLPA), and Spotrac’s own data aggregation and validation efforts. Spotrac compiles and cross-checks information through league sources and media reports to ensure accuracy. While Spotrac is not an official source affiliated with the NFL, it is a widely recognized and trusted aggregator of player contract and salary data, frequently cited by media outlets, sports analysts, and researchers.\nWhy was the data collected: Spotrac collects this data to provide transparency and insight into player contracts, salary cap figures, team payrolls, and other financial metrics relevant to fans, analysts, journalists, agents, and front offices. The goal is to make contract information publicly accessible and comparable across players and teams."
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "SQL",
    "section": "",
    "text": "Introduction:\nIn this project, I investigate racial disparities in police search behavior during traffic stops across six U.S. cities: Nashville, Charlotte, Saint Paul, New Orleans, and Austin, Using data from the Stanford Open Policing Project, I analyze whether Black and White drivers experience different search rates, and how those rates have changed over time. My approach involves querying each city’s data using SQL to summarize annual stop counts, search counts, and calculated search rates by race. I then compare these trends both within and across cities to identify patterns of disparity or improvement over time.\nExploratory analysis:\n\nlibrary(DBI)\nlibrary(RMariaDB)\n\n# Establish a secure connection to the 'traffic' database using environmental credentials\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n# List all available tables in the connected database\nDBI::dbListTables(con_traffic)\n\n [1] \"ar_little_rock_2020_04_01\"      \"az_gilbert_2020_04_01\"         \n [3] \"az_mesa_2023_01_26\"             \"az_statewide_2020_04_01\"       \n [5] \"ca_anaheim_2020_04_01\"          \"ca_bakersfield_2020_04_01\"     \n [7] \"ca_long_beach_2020_04_01\"       \"ca_los_angeles_2020_04_01\"     \n [9] \"ca_oakland_2020_04_01\"          \"ca_san_bernardino_2020_04_01\"  \n[11] \"ca_san_diego_2020_04_01\"        \"ca_san_francisco_2020_04_01\"   \n[13] \"ca_san_jose_2020_04_01\"         \"ca_santa_ana_2020_04_01\"       \n[15] \"ca_statewide_2023_01_26\"        \"ca_stockton_2020_04_01\"        \n[17] \"co_aurora_2023_01_26\"           \"co_denver_2020_04_01\"          \n[19] \"co_statewide_2020_04_01\"        \"ct_hartford_2020_04_01\"        \n[21] \"ct_statewide_2020_04_01\"        \"fl_saint_petersburg_2020_04_01\"\n[23] \"fl_statewide_2020_04_01\"        \"fl_tampa_2020_04_01\"           \n[25] \"ga_statewide_2020_04_01\"        \"ia_statewide_2020_04_01\"       \n[27] \"id_idaho_falls_2020_04_01\"      \"il_chicago_2023_01_26\"         \n[29] \"il_statewide_2020_04_01\"        \"in_fort_wayne_2020_04_01\"      \n[31] \"ks_wichita_2023_01_26\"          \"ky_louisville_2023_01_26\"      \n[33] \"ky_owensboro_2020_04_01\"        \"la_new_orleans_2020_04_01\"     \n[35] \"ma_statewide_2020_04_01\"        \"md_baltimore_2020_04_01\"       \n[37] \"md_statewide_2020_04_01\"        \"mi_statewide_2020_04_01\"       \n[39] \"mn_saint_paul_2020_04_01\"       \"mo_statewide_2020_04_01\"       \n[41] \"ms_statewide_2020_04_01\"        \"mt_statewide_2023_01_26\"       \n[43] \"nc_charlotte_2020_04_01\"        \"nc_durham_2020_04_01\"          \n[45] \"nc_fayetteville_2020_04_01\"     \"nc_greensboro_2020_04_01\"      \n[47] \"nc_raleigh_2020_04_01\"          \"nc_statewide_2020_04_01\"       \n[49] \"nc_winston_salem_2020_04_01\"    \"nd_grand_forks_2020_04_01\"     \n[51] \"nd_statewide_2020_04_01\"        \"ne_statewide_2020_04_01\"       \n[53] \"nh_statewide_2020_04_01\"        \"nj_camden_2020_04_01\"          \n[55] \"nj_statewide_2020_04_01\"        \"nv_henderson_2020_04_01\"       \n[57] \"nv_statewide_2020_04_01\"        \"ny_albany_2020_04_01\"          \n[59] \"ny_statewide_2020_04_01\"        \"oh_cincinnati_2020_04_01\"      \n[61] \"oh_columbus_2020_04_01\"         \"oh_statewide_2020_04_01\"       \n[63] \"ok_oklahoma_city_2023_01_26\"    \"ok_tulsa_2020_04_01\"           \n[65] \"or_statewide_2020_04_01\"        \"pa_philadelphia_2020_04_01\"    \n[67] \"ri_statewide_2020_04_01\"        \"sc_statewide_2020_04_01\"       \n[69] \"sd_statewide_2020_04_01\"        \"tn_nashville_2020_04_01\"       \n[71] \"tn_statewide_2020_04_01\"        \"tx_arlington_2020_04_01\"       \n[73] \"tx_austin_2020_04_01\"           \"tx_garland_2020_04_01\"         \n[75] \"tx_houston_2023_01_26\"          \"tx_lubbock_2020_04_01\"         \n[77] \"tx_plano_2020_04_01\"            \"tx_san_antonio_2023_01_26\"     \n[79] \"tx_statewide_2020_04_01\"        \"va_statewide_2020_04_01\"       \n[81] \"vt_burlington_2023_01_26\"       \"vt_statewide_2020_04_01\"       \n[83] \"wa_seattle_2020_04_01\"          \"wa_statewide_2020_04_01\"       \n[85] \"wa_tacoma_2020_04_01\"           \"wi_madison_2023_01_26\"         \n[87] \"wi_statewide_2020_04_01\"        \"wy_statewide_2020_04_01\"       \n\n\n\n# Query Nashville traffic stop data to calculate search rates for Black and White drivers by year\n\nSELECT \n  'Nashville' AS city,\n  subject_race AS race,\n  YEAR(date) AS year,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted) AS searches,\n  ROUND(SUM(search_conducted) * 1.0 / COUNT(*), 3) AS search_rate\nFROM tn_nashville_2020_04_01\nWHERE \n  subject_race IN ('black', 'white') AND\n  date IS NOT NULL AND\n  search_conducted IS NOT NULL\nGROUP BY subject_race, YEAR(date)\nORDER BY subject_race, YEAR(date);\n\n\n#| fig-alt: \"This chart shows two vertical boxplots, one for black drivers and one for white drivers in Nashville. The x-axis shows the driver’s race, and the y-axis shows the search rate, which means how often drivers were searched during traffic stops. Each box shows the range of search rates across multiple years. The line in the middle of the box is the median (typical) search rate, and the height of the box shows how much the rates vary. We see that the box representing black individuals is much higher that white individual which shows that, on average, black drivers are searched more frequently than white drivers\"\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# This code creates a boxplot comparing search rates for Black and White drivers in Nashville.\n# The search rate is calculated as the number of searches divided by the total number of stops, and is visualized as a distribution for each racial group across all years combined.\n\n\nnashville_df |&gt;\n  mutate(search_rate = searches / total_stops) |&gt;\n  ggplot(aes(x = race, y = search_rate, fill = race)) +\n  geom_boxplot() +\n  labs(\n    title = \"Overall Search Rate by Race in Nashville\",\n    x = \"Driver Race\",\n    y = \"Search Rate\",\n    fill = \"Driver Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows a Boxplot that plots the distribution of search rates for Black and White drivers in Nashville. Each box represents the total number of search rates within each racial group. The higher median for Black drivers suggests they were searched more frequently across all the years we have data forthan White drivers.\n\n\n# Query Charlotte traffic stop data to calculate search rates for Black and White drivers by year\n\nSELECT \n  'Charlotte' AS city,\n  subject_race AS race,\n  YEAR(date) AS year,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted) AS searches,\n  ROUND(SUM(search_conducted) * 1.0 / COUNT(*), 3) AS search_rate\nFROM tn_nashville_2020_04_01\nWHERE \n  subject_race IN ('black', 'white') AND\n  date IS NOT NULL AND\n  search_conducted IS NOT NULL\nGROUP BY subject_race, YEAR(date)\nORDER BY subject_race, YEAR(date);\n\n\n#| fig-alt: \"This chart shows two vertical boxplots, one for black drivers and one for white drivers in Charlotte The x-axis shows the driver’s race, and the y-axis shows the search rate, which means how often drivers were searched during traffic stops. Each box shows the range of search rates across multiple years. The line in the middle of the box is the median (typical) search rate, and the height of the box shows how much the rates vary. We see that the box representing black individuals is much higher that white individual which shows that, on average, black drivers are searched more frequently than white drivers\"\n\n# This code creates a boxplot comparing search rates for Black and White drivers in Charlotte\n# The search rate is calculated as the number of searches divided by the total number of stops, and is visualized as a distribution for each racial group across all years combined.\n\ncharlotte_df |&gt;\n  mutate(search_rate = searches / total_stops) |&gt;\n  ggplot(aes(x = race, y = search_rate, fill = race)) +\n  geom_boxplot() +\n  labs(\n    title = \"Overall Search Rate by Race in Charlotte\",\n    x = \"Driver Race\",\n    y = \"Search Rate\",\n    fill = \"Driver Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows a Boxplot that plots the distribution of search rates for Black and White drivers in Charlotte Each box represents the total number of search rates within each racial group. The higher median for Black drivers suggests they were searched more frequently across all the years we have data forthan White drivers.\n\n\n# Query Saint Paul traffic stop data to calculate search rates for Black and White drivers by year\n\nSELECT \n  'Saint Paul' AS city,\n  subject_race AS race,\n  YEAR(date) AS year,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted) AS searches,\n  ROUND(SUM(search_conducted) * 1.0 / COUNT(*), 3) AS search_rate\nFROM tn_nashville_2020_04_01\nWHERE \n  subject_race IN ('black', 'white') AND\n  date IS NOT NULL AND\n  search_conducted IS NOT NULL\nGROUP BY subject_race, YEAR(date)\nORDER BY subject_race, YEAR(date);\n\n\n#| fig-alt: \"This chart shows two vertical boxplots, one for black drivers and one for white drivers in Saint Paul The x-axis shows the driver’s race, and the y-axis shows the search rate, which means how often drivers were searched during traffic stops. Each box shows the range of search rates across multiple years. The line in the middle of the box is the median (typical) search rate, and the height of the box shows how much the rates vary. We see that the box representing black individuals is much higher that white individual which shows that, on average, black drivers are searched more frequently than white drivers\"\n\n# This code creates a boxplot comparing search rates for Black and White drivers in Saint Paul\n# The search rate is calculated as the number of searches divided by the total number of stops, and is visualized as a distribution for each racial group across all years combined.\n\nsaint_paul_df |&gt;\n  mutate(search_rate = searches / total_stops) |&gt;\n  ggplot(aes(x = race, y = search_rate, fill = race)) +\n  geom_boxplot() +\n  labs(\n    title = \"Overall Search Rate by Race in Saint Paul\",\n    x = \"Driver Race\",\n    y = \"Search Rate\",\n    fill = \"Driver Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows a Boxplot that plots the distribution of search rates for Black and White drivers in Saint Paul Each box represents the total number of search rates within each racial group. The higher median for Black drivers suggests they were searched more frequently across all the years we have data forthan White drivers.\n\n\n# Query New Orleans traffic stop data to calculate search rates for Black and White drivers by year\n\nSELECT \n  'New Orleans' AS city,\n  subject_race AS race,\n  YEAR(date) AS year,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted) AS searches,\n  ROUND(SUM(search_conducted) * 1.0 / COUNT(*), 3) AS search_rate\nFROM tn_nashville_2020_04_01\nWHERE \n  subject_race IN ('black', 'white') AND\n  date IS NOT NULL AND\n  search_conducted IS NOT NULL\nGROUP BY subject_race, YEAR(date)\nORDER BY subject_race, YEAR(date);\n\n\n#| fig-alt: \"This chart shows two vertical boxplots, one for black drivers and one for white drivers in New Orleans The x-axis shows the driver’s race, and the y-axis shows the search rate, which means how often drivers were searched during traffic stops. Each box shows the range of search rates across multiple years. The line in the middle of the box is the median (typical) search rate, and the height of the box shows how much the rates vary. We see that the box representing black individuals is much higher that white individual which shows that, on average, black drivers are searched more frequently than white drivers\"\n\n# This code creates a boxplot comparing search rates for Black and White drivers in New Orleans\n# The search rate is calculated as the number of searches divided by the total number of stops, and is visualized as a distribution for each racial group across all years combined.\n\nnew_orleans_df |&gt;\n  mutate(search_rate = searches / total_stops) |&gt;\n  ggplot(aes(x = race, y = search_rate, fill = race)) +\n  geom_boxplot() +\n  labs(\n    title = \"Overall Search Rate by Race in New Orleans\",\n    x = \"Driver Race\",\n    y = \"Search Rate\",\n    fill = \"Driver Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows a Boxplot that plots the distribution of search rates for Black and White drivers in New Orleans Each box represents the total number of search rates within each racial group. The higher median for Black drivers suggests they were searched more frequently across all the years we have data forthan White drivers.\n\n\n# Query Austin traffic stop data to calculate search rates for Black and White drivers by year\n\nSELECT \n  'Austin' AS city,\n  subject_race AS race,\n  YEAR(date) AS year,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted) AS searches,\n  ROUND(SUM(search_conducted) * 1.0 / COUNT(*), 3) AS search_rate\nFROM tn_nashville_2020_04_01\nWHERE \n  subject_race IN ('black', 'white') AND\n  date IS NOT NULL AND\n  search_conducted IS NOT NULL\nGROUP BY subject_race, YEAR(date)\nORDER BY subject_race, YEAR(date);\n\n\n#| fig-alt: \"This chart shows two vertical boxplots, one for black drivers and one for white drivers in Austin The x-axis shows the driver’s race, and the y-axis shows the search rate, which means how often drivers were searched during traffic stops. Each box shows the range of search rates across multiple years. The line in the middle of the box is the median (typical) search rate, and the height of the box shows how much the rates vary. We see that the box representing black individuals is much higher that white individual which shows that, on average, black drivers are searched more frequently than white drivers\"\n\n# This code creates a boxplot comparing search rates for Black and White drivers in Austin\n# The search rate is calculated as the number of searches divided by the total number of stops, and is visualized as a distribution for each racial group across all years combined.\n\naustin_df |&gt;\n  mutate(search_rate = searches / total_stops) |&gt;\n  ggplot(aes(x = race, y = search_rate, fill = race)) +\n  geom_boxplot() +\n  labs(\n    title = \"Overall Search Rate by Race in Austin\",\n    x = \"Driver Race\",\n    y = \"Search Rate\",\n    fill = \"Driver Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows a Boxplot that plots the distribution of search rates for Black and White drivers in Austin Each box represents the total number of search rates within each racial group. The higher median for Black drivers suggests they were searched more frequently across all the years we have data forthan White drivers.\n\n\n#This query extracts and combines stop and search counts by race and year for Black and White drivers in Nashville, Charlotte, Saint Paul, New Orleans, and Austin.\n\nSELECT 'Nashville' AS city, subject_race AS race, YEAR(date) AS year,\n       COUNT(*) AS total_stops,\n       SUM(CASE WHEN search_conducted = TRUE THEN 1 ELSE 0 END) AS searches\nFROM tn_nashville_2020_04_01\nWHERE subject_race IN ('black', 'white')\nGROUP BY subject_race, YEAR(date)\n\nUNION ALL\n\nSELECT 'Charlotte' AS city, subject_race AS race, YEAR(date) AS year,\n       COUNT(*) AS total_stops,\n       SUM(CASE WHEN search_conducted = TRUE THEN 1 ELSE 0 END) AS searches\nFROM nc_charlotte_2020_04_01\nWHERE subject_race IN ('black', 'white')\nGROUP BY subject_race, YEAR(date)\n\nUNION ALL\n\nSELECT 'Saint Paul' AS city, subject_race AS race, YEAR(date) AS year,\n       COUNT(*) AS total_stops,\n       SUM(CASE WHEN search_conducted = TRUE THEN 1 ELSE 0 END) AS searches\nFROM mn_saint_paul_2020_04_01\nWHERE subject_race IN ('black', 'white')\nGROUP BY subject_race, YEAR(date)\n\nUNION ALL\n\nSELECT 'New Orleans' AS city, subject_race AS race, YEAR(date) AS year,\n       COUNT(*) AS total_stops,\n       SUM(CASE WHEN search_conducted = TRUE THEN 1 ELSE 0 END) AS searches\nFROM la_new_orleans_2020_04_01\nWHERE subject_race IN ('black', 'white')\nGROUP BY subject_race, YEAR(date)\n\nUNION ALL\n\nSELECT 'Austin' AS city, subject_race AS race, YEAR(date) AS year,\n       COUNT(*) AS total_stops,\n       SUM(CASE WHEN search_conducted = TRUE THEN 1 ELSE 0 END) AS searches\nFROM tx_austin_2020_04_01\nWHERE subject_race IN ('black', 'white')\nGROUP BY subject_race, YEAR(date);\n\n\n#| fig-alt: \"This chart shows a series of small line graphs, one for each city. Each small graph has two colored lines — one for Black drivers and one for White drivers. The x-axis shows the year, and the y-axis shows the search rate, which is the percentage of traffic stops that led to a search. The lines show how search rates changed over time for each race within each city. We see that the black driver line is consistently higher than the white driver line meaning black people are searched more often.\"\n\n# Create a line plot showing the proportion of stops that resulted in a search by race and city across years, faceted by city\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndf |&gt;\n  mutate(year = as.numeric(year)) |&gt;\n  ggplot(aes(x = year, y = searches / total_stops, color = race)) +\n  geom_line(size = 1.2) +\n  facet_wrap(~ city) +\n  labs(title = \"Search Rate Over Time by Race and City\",\n       x = \"Year\", y = \"Search Rate\",\n       color = \"Driver Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays search rates over time for Black and White drivers in each of the six cities included in the analysis seperated into 6 graphs per city. Each line represents the percentage of stops that resulted in a search for each racial group, by year. Across all cities, Black drivers consistently experienced higher search rates than White drivers. The gap between the two groups is visible in most cities and remains relatively stable over time, suggesting a persistent racial disparity in search practices.\n\n#| fig-alt: \"This chart shows side-by-side bars for Black and White drivers in each of five cities. The x-axis shows the cities, and the y-axis shows the total number of traffic stops. Each city has two bars next to each other—one for each race—so you can directly compare how often each group was stopped in that city. we actually don't see a trend in which race gets stopped more so we know that Black drivers are searched at a hgiher rate not because they are stopped more but likely for other reasons such as racial bias.\"\n\n# This code creates a bar chart showing how many traffic stops occurred for Black and White drivers in each of five cities.\n\ndf |&gt;\n  group_by(city, race) |&gt;\n  summarise(total_stops = sum(total_stops)) |&gt;\n  ggplot(aes(x = city, y = total_stops, fill = race)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Total Stops by Race and City\",\n       x = \"City\", y = \"Total Stops\", fill = \"Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis bar chart compares the total number of stops for Black and White drivers in each city over the full time period covered by the data. It shows this by having total stops on the y-axis and cities on the x-axis. We can see two bars for each city to show how many times black people were stopped and how many times white people were stopped. While some cities show more stops for White drivers and others for Black drivers, this variation likely reflects differences in each city’s racial demographics rather than disparities in policing alone. It could also be attributed to having some incomplete or less data in some years This plot provides important context for interpreting the search rate data by showing the baseline number of stops per group.\nConclusion: Through querying traffic stop data across six U.S. cities using SQL, I analyzed how search rates compare between Black and White individuals over multiple years. By combining city-level results into a unified dataset, I was able to observe trends within and across locations, comparing both the rate and total number of searches by race. The analysis revealed that Black drivers consistently experienced higher search rates than White drivers in every city studied. While the magnitude of disparity varied by city and year, the pattern was both persistent and systemic. Interestingly, when comparing the total number of searches, the dominant racial group varied by city—likely reflecting differences in local demographic composition rather than search behavior alone.\nCitation: Pierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10.\nWho collected the data? The data was collected and analyzed by a multidisciplinary research team led by Emma Pierson, with co-authors from Stanford University, including members of the Stanford Computational Policy Lab and other researchers from academia and civic technology.\nWhy? The data was gathered to conduct one of the largest empirical investigations into racial disparities in police traffic stops in the United States. The study sought to measure disparities in stop rates, search decisions, and outcomes, and to test whether these disparities could be explained by differences in behavior versus systemic bias.\nOriginal source: The underlying stop data was obtained from publicly available datasets released by state and local law enforcement agencies across the U.S., covering over 100 million traffic stops. The Stanford Open Policing Project (https://openpolicing.stanford.edu) played a central role in assembling and standardizing this data for research use."
  },
  {
    "objectID": "FastFood.html",
    "href": "FastFood.html",
    "title": "FastFood",
    "section": "",
    "text": "Introduction:\nTo understand how caloric content varies across different fast food restaurants, I analyzed a data set containing menu item information from multiple chains. After importing the data, I focused on the calories variable and used ggplot2 to create box plots of calorie counts for each restaurant. By using facet_wrap, I generated separate plots for each chain, allowing for side-by-side comparison of the distribution of calories per item. This visualization helps identify which restaurants tend to offer higher-calorie foods and which ones have more balanced or lower-calorie options.\nExploratory Analysis:\n\n#| fig-alt: \"Each restaurant is listed vertically along the side. Horizontal bars extend to the right — the longer the bar, the more menu items that restaurant has. The longest bar belongs to Mcdonald’s, meaning it has the most food items in the dataset. The shortest bars belong to Arby’s and Chick-fil-A, showing they have fewer items. This chart helps you visually compare how much each restaurant contributes to the data.\"\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Load the data\nFast_Food &lt;- read_excel(\"Fast Food.xlsx\")\n\n# Bar chart: number of menu items per restaurant\nFast_Food |&gt;\n  count(restaurant) |&gt;\n  ggplot(aes(x = reorder(restaurant, n), y = n, fill = restaurant)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(\n    title = \"Number of Menu Items by Restaurant\",\n    x = \"Restaurant\",\n    y = \"Number of Menu Items\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe bar chart shows how many menu items each fast food restaurant has in the data set. The y-axis lists the restaurant names, and the x-axis shows the number of menu items. McDonald’s has the longest bar, meaning it has the most items in the data set, followed by Burger King. Chick-fil-A and Arby’s have shorter bars, indicating they have fewer items. This chart helps us understand how much each restaurant contributes to the data before we analyze calorie content.\nVariables:\n\nrestaurant: Name of the fast food chain (e.g., McDonald’s, Burger King).\nitem: Specific menu item.\ncalories: Total calories in the item.\ncal_fat: Calories from fat.\ntotal_fat: Total fat in grams.\nsaturated_fat: Saturated fat in grams.\ntrans_fat: Trans fat in grams.\ncholesterol: Cholesterol in milligrams.\nsodium: Sodium in milligrams.\ntotal_carb: Total carbohydrates in grams.\nfiber: Dietary fiber in grams.\nsugar: Sugars in grams.\nprotein: Protein in grams.\nvit_a: Vitamin A percentage of daily value.\nvit_c: Vitamin C percentage of daily value.\ncalcium: Calcium percentage of daily value.\nsalad: Indicator if the item is a salad (TRUE/FALSE).\n\n\n#| fig-alt: \"This chart shows one box for each fast food restaurant, placed in its own panel. Each box represents the range of calorie values for that restaurant’s menu items. The horizontal axis shows calories, while each restaurant’s panel helps you compare them side by side. The line inside each box shows the median (typical) calorie count, and the length of the box shows how spread out the calorie values are. Some restaurants, like McDonald’s and Burger King, have wider boxes or more outliers, meaning their menus have a bigger variety of high- and low-calorie items. Others, like Chick-fil-A, have more tightly grouped calorie counts.\" \n\n# Create a boxplot to visualize the distribution of calories for each restaurant\nggplot(Fast_Food, aes(x = calories)) +\n  geom_boxplot(color = \"darkred\")+\n  facet_wrap(~restaurant)+\n  labs(title = \"Calorie distribution of each restaurant's menu\")\n\n\n\n\n\n\n\n\nThis set of box plots displays the distribution of calorie counts for menu items at various fast food restaurants. Each panel represents a different chain, showing the spread of calories offered on their menus. The boxes illustrate the interquartile range (middle 50% of items), the horizontal line inside each box shows the median calorie count, and dots (if any) highlight outlier items with unusually high or low calories. This visual makes it easy to compare how calorie-dense different menus are across fast food brands.\nExtra Credit:\n\n#| fig-alt: \"This chart shows a box for each restaurant, comparing the range of calories across their menu items. The x-axis lists the restaurants, and the y-axis shows the calorie values. Each box displays how calorie counts vary within a restaurant. The middle line in each box shows the median (typical) calorie count, and the height of the box shows how spread out the items are. You can hover over each box to see exact values, making it easy to explore differences. Restaurants like McDonald’s and Burger King have higher and more spread-out calorie ranges, while others like Chick-fil-A have tighter, lower distributions.\"\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(plotly)\n\n# Read the data\nFast_Food &lt;- read_excel(\"Fast Food.xlsx\")\n\n# Create corrected boxplot\np &lt;- ggplot(Fast_Food, aes(x = restaurant, y = calories)) +\n  geom_boxplot(color = \"darkred\") +\n  labs(title = \"Calorie Distribution by Restaurant\",\n       x = \"Restaurant\", y = \"Calories\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert to interactive plot\ninteractive_plot &lt;- ggplotly(p)\ninteractive_plot\n\n\n\n\n\nFor my extra credit, I improved my original fast food calorie graph by turning it from a regular static chart into an interactive one using Plotly. My original graph used box plots to show how calories are spread out across different restaurants, which made it easy to spot things like medians and outliers. But the static version had some downsides in that it didn’t let viewers see exact values or easily compare restaurants, especially when the data overlapped. By switching to Plotly, I made the chart interactive so users can now hover over each box to see specific calorie numbers, zoom in on certain restaurants, and explore the data in more detail. This makes the chart easier to use and more interesting to look at. It also helps people better understand the differences in calorie content between restaurants by letting them explore the data on their own.\nCitations\nTidyTuesday source: Hughes, Ellis ., & the R for Data Science Online Learning Community. (2018, September 4). Fast Food Calories – TidyTuesday Dataset. Retrieved from https://github.com/rfordatascience/tidytuesday/tree/main/data/2018/2018-09-04\n\nWho collected the data? The data was collected by Ellis Hughes and the R for Data Science Online Learning Community as part of the TidyTuesday project.\nIt was collected to provide a real-world data set for learners to practice data analysis, visualization, and coding skills in R.\nOriginal source of the information: The original data comes from a data set hosted on Kaggle, which compiled nutrition facts from the menus of major fast food chains.\n\nOriginal data source: Fast Food Nutrition. (2025, May 6). McDonald’s Nutrition Facts & Calories. Retrieved from https://fastfoodnutrition.org/mcdonalds\n\nWho collected the data? The data was compiled by Fast Food Nutrition, an independent website that aggregates nutritional information from various fast-food chains.\nWhy it was collected? To provide consumers with accessible and comprehensive nutritional information for menu items at fast-food restaurants, aiding in informed dietary choices.\nOriginal source of the information: The nutritional data originates from all the restaurant’s official nutritional disclosures and publicly available information."
  },
  {
    "objectID": "Final Presentation.html#introduction-methodology",
    "href": "Final Presentation.html#introduction-methodology",
    "title": "Final Presentation",
    "section": "Introduction & Methodology",
    "text": "Introduction & Methodology\nResearch Question: Does greater access to financial institutions correlate with higher national wealth, as measured by GDP per capita?\nData Overview:\n\n120 countries\nVariables sourced from World Bank/Wolrdometer\nCountries grouped by median GDP per capita:\n\nHigh GDP group: Above median\nLow GDP group: Below median\n\n\nMain Tool:\n\nPermutation Test\nSimulates the null hypothesis: no relationship between GDP and account access\n1,000 iterations with random shuffling"
  },
  {
    "objectID": "Final Presentation.html#data-preparation",
    "href": "Final Presentation.html#data-preparation",
    "title": "Final Presentation",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nGDP_analysis &lt;- read_excel(\"GDP_analysis.xlsx\", sheet = \"Data\", skip = 1)\ncolnames(GDP_analysis) &lt;- c(\n  \"Country\", \n  \"GDP_Per_Capita\", \n  \"Account_15plus\", \n  \"Saved_15plus\", \n  \"Borrowed_Formal_15plus\", \n  \"Account_Female\", \n  \"Account_Male\", \n  \"Owns_Credit_Card\")\n\nDP_analysis &lt;- GDP_analysis |&gt; \n  mutate(across(GDP_Per_Capita:Owns_Credit_Card, as.numeric))\n  \nmedian_gdp &lt;- median(GDP_analysis$GDP_Per_Capita, na.rm = TRUE)\n\nGDP_analysis &lt;- GDP_analysis |&gt;  \n  mutate(GDP_Group = ifelse(GDP_Per_Capita &gt;= median_gdp, \"High\", \"Low\"))"
  },
  {
    "objectID": "Final Presentation.html#box-plot-visualization",
    "href": "Final Presentation.html#box-plot-visualization",
    "title": "Final Presentation",
    "section": "Box plot Visualization",
    "text": "Box plot Visualization\n\nGDP_analysis |&gt;\n  ggplot(aes( x = GDP_Group, y = Account_15plus )) +\n  geom_boxplot()"
  },
  {
    "objectID": "Final Presentation.html#permutation-test",
    "href": "Final Presentation.html#permutation-test",
    "title": "Final Presentation",
    "section": "Permutation Test",
    "text": "Permutation Test\nNull hypothesis:\nThere is no difference in the degree of access to financial institutions between countries with higher and lower GDP per capita.\nAlternate Hypothesis:\nCountries with a higher GDP per capita have greater access to financial institutions than those with a lower GDP per capita.\n\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    select(GDP_Group, Account_15plus) |&gt;\n    mutate(Account_15plus_perm = sample(Account_15plus, replace = FALSE)) %&gt;%\n    group_by(GDP_Group) |&gt;\n    summarize(\n      obs_ave = mean(Account_15plus, na.rm = TRUE),\n      obs_med = median(Account_15plus, na.rm = TRUE),\n      perm_ave = mean(Account_15plus_perm, na.rm = TRUE),\n      perm_med = median(Account_15plus_perm, na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt;\n    summarize(\n      obs_ave_diff = diff(obs_ave),\n      obs_med_diff = diff(obs_med),\n      perm_ave_diff = diff(perm_ave),\n      perm_med_diff = diff(perm_med),\n      rep = rep\n    )\n}\n\nperm_results &lt;- map_df(1:1000, perm_data, data = GDP_analysis)\n\nhead(perm_results)\n\n# A tibble: 6 × 5\n  obs_ave_diff obs_med_diff perm_ave_diff perm_med_diff   rep\n         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1       -0.459       -0.572       -0.0636       -0.193      1\n2       -0.459       -0.572       -0.110        -0.276      2\n3       -0.459       -0.572        0.0172        0.0616     3\n4       -0.459       -0.572       -0.0179        0.0289     4\n5       -0.459       -0.572        0.0331        0.0488     5\n6       -0.459       -0.572        0.0446        0.148      6"
  },
  {
    "objectID": "Final Presentation.html#permutation-test-results",
    "href": "Final Presentation.html#permutation-test-results",
    "title": "Final Presentation",
    "section": "Permutation test results",
    "text": "Permutation test results\n\nggplot(perm_results, aes(x = perm_ave_diff)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(obs_ave_diff)), color = \"red\") +\n  labs(\n    title = \"Permutation Test: Difference in Means of Account Ownership (15+)\",\n    x = \"Difference in Means (Low GDP - High GDP)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(perm_results, aes(x = perm_med_diff)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(obs_med_diff)), color = \"blue\",) +\n  labs(\n    title = \"Permutation Test: Difference in Medians of Account Ownership (15+)\",\n    x = \"Difference in Medians (Low GDP - High GDP)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nathan Dhanani",
    "section": "",
    "text": "Hello! My name is Nathan Dhanani and I am an Economic/Finance student at Claremont McKenna College. I am hoping to be an Economic Consultant and am currently interning at Analysis Group. I love playing soccer, lifting weights, pickle ball and Spikeball. Feel free to browse my website to learn more."
  },
  {
    "objectID": "Stinganalysis.html",
    "href": "Stinganalysis.html",
    "title": "StringAnalysis",
    "section": "",
    "text": "Introduction:\nIn today’s media landscape, the New York Times remains one of the most influential news outlets, shaping public discourse on politics and international affairs. To better understand how political topics are covered in NYT headlines, this analysis explores a dataset of article titles, focusing on mentions of key political terms and global locations. By identifying headlines that reference terms like “election,” “president,” or “congress,” and extracting country or region names, we can begin to map out how political attention is distributed across the globe and how it has evolved throughout time. This exploration provides insight into the geographic focus of political reporting, how frequently certain regions appear in headlines, and how this coverage has changed over time. Through a combination of text filtering and location extraction, the following analysis aims to uncover patterns in political narrative emphasis and international visibility in the New York Times.\nExploratory Analysis:\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RTextTools)\ndata(\"NYTimes\")\n\n\n#| fig-alt: \"The chart displays vertical bars, one for each year. The x-axis lists the years covered in the dataset, and the y-axis shows how many articles were published in each year. Taller bars mean more articles. This lets you easily see which years have more content and whether article volume increased, decreased, or stayed steady over time. The amoutn fo articles stayed consistent around 300 until 2003 where it drops to about 250 articles\"\n\n# Convert date and extract year\nNYTimes |&gt;\n  mutate(Date = dmy(Date), Year = year(Date)) |&gt;\n  count(Year) |&gt;\n  ggplot(aes(x = Year, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Number of NYTimes Articles per Year\",\n    x = \"Year\",\n    y = \"Article Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis bar chart shows the number of New York Times articles included in the dataset for each year. It gives an overview of the time span covered by the data and how many articles were published annually. This helps us understand the distribution of article volume over time, which is useful context before filtering or analyzing specific topics like politics or geography.\nVariables:\n\nArticle_ID - A unique numerical identifier for each article in the dataset\nDate - The publication date of the article (usually in day-month-year format)\nTitle - The article headline as it appeared in The New York Times\nSubject - A brief summary or annotation describing the main idea of the article\nTopic.Code - A numeric code representing the article’s assigned policy topic or issue domain\n\n\n# Load the NYTimes data set and convert it to a tibble for easier handling\ndata(NYTimes)\nas_tibble(NYTimes)\n\n# A tibble: 3,104 × 5\n   Article_ID Date      Title                                 Subject Topic.Code\n        &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                                 &lt;fct&gt;        &lt;int&gt;\n 1      41246 1-Jan-96  Nation's Smaller Jails Struggle To C… Jails …         12\n 2      41257 2-Jan-96  FEDERAL IMPASSE SADDLING STATES WITH… Federa…         20\n 3      41268 3-Jan-96  Long, Costly Prelude Does Little To … Conten…         20\n 4      41279 4-Jan-96  Top Leader of the Bosnian Serbs Now … Bosnia…         19\n 5      41290 5-Jan-96  BATTLE OVER THE BUDGET: THE OVERVIEW… Battle…          1\n 6      41302 7-Jan-96  South African Democracy Stumbles on … politi…         19\n 7      41314 8-Jan-96  Among Economists, Little Fear on Def… econom…          1\n 8      41333 10-Jan-96 BATTLE OVER THE BUDGET: THE OVERVIEW… budget…          1\n 9      41344 11-Jan-96 High Court Is Cool To Census Change   census…         20\n10      41355 12-Jan-96 TURMOIL AT BARNEYS: THE DIFFICULTIES… barney…         15\n# ℹ 3,094 more rows\n\n\n\n# This chunk filters articles that mention political topics (e.g. \"election\", \"president\", \"congress\") and extracts geographic keywords. It then counts how many political headlines include each location.\n\nNYTimesv2 &lt;- NYTimes |&gt;\n  mutate(politics_mention = str_detect(Title,regex(\"election|vote|senate|president|congress|politics|congress|clinton|trump\", ignore_case = TRUE))) |&gt;\n  mutate(title_word_count = str_count(Title,\"\\\\S+\")) |&gt;\n  mutate(location = str_extract(Title, regex(\"New York|California|Texas|France|Russia|China|Japan|Mexico|Bosnia|Serbia|Germany|UK|USA\", ignore_case = TRUE)))|&gt;\n  mutate(location = str_to_lower(location)) |&gt;\n  filter(politics_mention == TRUE, !is.na(location)) |&gt;\n  group_by(location) |&gt;\n  summarize(article_count = n(), .groups = \"drop\")|&gt;\n    mutate(location = fct_reorder(location, article_count))  \n\n\n#| fig-alt: \"This chart uses vertical bars to show which locations were mentioned most often in political news articles. The x-axis lists different locations, and the y-axis shows how many articles mentioned each one. Each bar’s height shows how frequently that location appeared and how blue they are represents the article count.From right to left, the bars get shorter and lose their blue color to represent a decreasing number of articles. China and Mexico are most frequently mentioned\"\n\n# This chunk creates a Bar chart which depicts the most mentioned locations in political articles\nggplot(NYTimesv2, aes(x = location, y = article_count, fill = article_count)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Most Mentioned Locations in Political Articles\",\n       x = \"Location\",\n       y = \"Number of Articles\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis bar chart shows which geographic locations appear most frequently in political articles published by The New York Times. Each bar represents a specific location mentioned in political headlines, based on keywords like “election,” “vote,” “president,” etc. The data has been filtered to include only articles that reference both politics and one of several predefined countries or regions. The color gradient indicates the volume of mentions, making it easier to spot which places dominate the political conversation in NYT headlines.\n\n#In this chunk, we extract the year from each article date and count how many political headlines occurred each year. This helps us track The New York Times political coverage volume over time.\n\nlibrary(lubridate)\nNYTimesv3 &lt;- NYTimes |&gt;\n  mutate(Date = dmy(Date),  \n         Year = year(Date),\n         politics_mention = str_detect(Title, regex(\"election|vote|senate|president|congress|politics|congress|clinton|trump\", ignore_case = TRUE)),\n         title_word_count = str_count(Title,\"\\\\S+\"),\n         location = str_extract(Title, regex(\"New York|California|Texas|France|Russia|China|Japan|Mexico|Bosnia|Serbia|Germany|UK|USA\", ignore_case = TRUE))) |&gt;\n  filter(politics_mention == TRUE, !is.na(location)) |&gt;\n  group_by(Year) |&gt;\n  summarise(article_count = n(), .groups = \"drop\") |&gt;\n  arrange(Year)\n\n\n#| fig-alt: \"Line chart showing the number of political articles in the NYTimes each year that mention major global locations. The line shows an overall decrease in political menioning but fluctuates in time periods where their is increasing and decreasing amount of political mentions in articles.\"\n\n#In this chunk, we create a line chart that counts the amount of political article frequency by year\n\nggplot(NYTimesv3, aes(x = Year, y = article_count)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(size = 2, color = \"red\") +\n  labs(title = \"Political Articles Mentioning Key Locations Over Time\",\n       x = \"Year\",\n       y = \"Number of Articles\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis line chart tracks the number of political articles published each year by The New York Times that mention at least one major global location. By focusing only on political headlines containing specific keywords and country names, this plot visualizes how global political focus in NYT coverage has shifted or intensified over time. Spikes or dips in the line may correspond to key events such as presidential elections, international conflicts, or geopolitical crises.\nCitation:\nBoydstun, A. E. (2013). Supplementary Information for: Making the News: Politics, the Media, and Agenda Setting. Retrieved from http://www.amber-boydstun.com/supplementary-information-for-making-the-news.html\nAdditional citation context:\nWho collected the data? The dataset was compiled by Dr. Amber E. Boydstun, a political scientist and professor at the University of California, Davis. She gathered this data as part of her research for the book Making the News: Politics, the Media, and Agenda Setting, which examines patterns in media attention and agenda-setting processes.\nWhy it was collected? The primary aim of collecting this data was to analyze how policy issues gain prominence in media coverage, particularly focusing on the dynamics of news reporting by major outlets like The New York Times. The research sought to understand the factors influencing which issues are highlighted in the news and how this affects public discourse and policy-making.\nOriginal source of the information: The data originates from a systematic content analysis of The New York Times front-page articles spanning from 1996 to 2006. Each article was coded for various attributes, including policy topics, framing, and prominence, to facilitate a comprehensive study of media coverage patterns."
  }
]