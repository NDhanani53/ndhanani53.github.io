---
title: "DataEthics"
description: |
 [ProPublica article link](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
 [Wired article link](https://www.wired.com/story/crime-predicting-algorithms-may-not-outperform-untrained-humans/)
author: Nathan Dhanani
date: April 16, 2025
format: html
---

Introduction:

Imagine you get caught committing a petty crime, such as stealing someone’s purse out of their car, and you find yourself in court negotiating your parole and sentence with a judge. During this delegation, you are asked to fill out a test of 137 questions to predict how dangerous you may be in the future and whether you will get parole. Now, imagine you are a person of color: you may find that this algorithm predicted you to be much more dangerous in the future than a white individual who did something similar or worse, and you find yourself with a harsher sentence than that white individual. Also, imagine you learn information about previous offenses and data on you is being fed to this algorithm without your consent—an algorithm designed to determine how to rehabilitate people, not how to sentence them.

This happens quite often with the use of a predictive algorithm called COMPAS, which analyzes answers to 137 questions, ranging from criminal history to personal background (like whether your parents were incarcerated or if you have a job), to assess your risk level. This proprietary tool is designed to forecast the likelihood of a criminal defendant reoffending and is used in several stages of the U.S. criminal justice system, including pretrial release decisions and sentencing. COMPAS generates risk scores by analyzing answers to 137 items derived from both personal interviews and criminal records. 

The ProPublica analysis, which was used in the article to evaluate whether the tool held racial bias, involved collecting real-world risk scores from over 7,000 individuals arrested in Broward County, Florida. They then checked their actual criminal records over the following two years. They used this data to statistically evaluate the accuracy and fairness of the COMPAS predictions. While the tool was about 61% accurate overall (slightly better than random guessing), ProPublica found that it made systematic errors based on race: it overestimated the risk for Black defendants and underestimated it for white defendants.

This poses an ethical dilemma in that this algorithm has been found to produce biased results. ProPublica’s investigation showed that Black defendants were far more likely to be labeled high risk even when they did not reoffend, while white defendants with worse records were often labeled low risk. These scores can and have deeply influenced people’s freedom, yet the individuals affected don’t know how the scores are calculated and have limited ability to challenge them. This raises serious concerns about transparency, fairness, and racial bias in automated decision-making systems used by the justice system.

What was the consent structure for recruiting participants? Were the participants aware of the ways their data would be used for research? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen?

The core issue here is informed consent, which requires that participants understand how their data will be used and voluntarily agree to those uses. In the case of COMPAS, individuals were not recruited as research participants, but they were criminal defendants whose data was collected to determine the proceedings of their court cases. As highlighted in the ProPublica investigation, these defendants were likely unaware that their answers to personal questions, such as if a parent had been to prison, would later be used in said proceedings as well as journalism and academic research. The WIRED article by Lapowsky supports this concern by showing how these data, originally collected for case processing, were later used to evaluate algorithmic fairness and predictive accuracy, often without any consent from the individuals involved. This raises serious questions about whether informed consent is even possible in a setting like the criminal justice system. More broadly, both articles illustrate how difficult it is to secure meaningful consent for future, and other unforeseen uses, especially when those uses include third-party audits, news reports, or machine learning research.

Is the data being used in unintended ways to the original study?

The core issue here is mitigating bias in ourselves and the data we apply to the real world, especially when we are applying it to the freedom of others. This involves identifying and correcting disparities in how predictive models treat different demographic groups. The ProPublica article exposed how COMPAS produced racially skewed predictions: Black defendants were far more likely than white defendants to be falsely labeled as high-risk. Even though the model did not explicitly use race, it incorporated proxies, such as education, employment, and family history, that have underlying systemic racial inequalities. The WIRED article by Lapowsky extends this point, showing how Dartmouth researchers compared COMPAS to predictions made by Mechanical Turk workers and found similar racial disparities in both cases, despite omitting race as a data point. This suggests that these unintended biases are an underlying problem in the algorithm. The fact that COMPAS was used beyond its original scope which is influencing sentencing decisions without reevaluation or transparency, further shows the unintended uses of data which continue to exacerbate structural inequalities. 

Should race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender?

The core issue of this question is to consider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society. It's important to recognize if race should be used as a variable in this study or if it is used as a proxy for something else. According to ProPublica, COMPAS does not use race directly, but the model’s reliance on variables like prior convictions, family incarceration, and employment effectively makes race a proxy. The WIRED article confirms that even when race was withheld from human participants and machine learning models, the false positive rate remained higher for Black defendants, suggesting that race is indirectly encoded through other features. This shows that omitting race doesn’t remove bias but makes it less visible. The Dartmouth study also revealed that only two variables age and number of prior offenses were needed to match COMPAS’s accuracy, but these too reflect racial disparities in policing and prosecution. Gender operates similarly: while it may appear neutral, its inclusion can reinforce stereotypes and institutional inequities unless handled with care. Together, these articles argue that merely excluding race or gender does not ensure fairness but in fact, it may prevent the detection and correction of embedded bias. An interesting thought could be to add race into these models to in a sense dilute the consideration of other variables such as family incarceration and employment. Systematic inequalities make these attributes more common in Black Americans and if we can recognize that in the model we could weigh those variables less harshly. However, this could backfire and make the model more racist.

How were the variables collected? Were they accurately recorded? Is there any missing data?

The core issue here is data integrity, ensuring that the variables used to power predictive algorithms are accurate, complete, and reliable. In ProPublica’s investigation, the COMPAS tool drew from self-reported questionnaires and criminal records, including subjective questions like one’s attitudes toward authority or peer behavior. This raises questions about the validity of responses gathered especially in an institutional setting. The WIRED article echoes these concerns by pointing out that COMPAS amasses up to 137 data points per defendant, many of which are unverifiable or not publicly documented. Furthermore, Equivant, the company behind COMPAS, has never disclosed the full list of variables or their weights, making external auditing difficult. This opacity prevents researchers from checking for missing data, measurement error, or inconsistency in data collection methods. According to the WIRED piece, even when researchers built a simplified model with just seven variables, it performed similarly to COMPAS suggesting that the extra data may not add meaningful accuracy and could introduce more bias or error. Without transparency, there is no way to ensure that the model’s decisions are based on solid evidence rather than flawed inputs.

Citations:

Angwin, Julia, et al. "Machine Bias: There’s Software Used across the Country to Predict Future Criminals. And It’s Biased against Blacks." ProPublica, 23 May 2016, www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.

Lapowsky, Issie. "Crime-Predicting Algorithms May Not Fare Much Better Than Untrained Humans." WIRED, 17 Jan. 2018, https://www.wired.com/story/crime-predicting-algorithms-may-not-outperform-untrained-humans/.
