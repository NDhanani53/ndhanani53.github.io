---
title: "DataEthics"
description: |
 [ProPublica article link](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
 [Wired article link](https://www.wired.com/story/crime-predicting-algorithms-may-not-outperform-untrained-humans/)
author: Nathan Dhanani
date: April 16, 2025
format: html
---

Introduction:

Imagine you get caught committing a petty crime, such as stealing someone’s purse out of their car, and you find yourself in court negotiating your parole and sentence with a judge. During this delegation, you are asked to fill out a test of 137 questions to predict how dangerous you may be in the future and whether you will get parole. Now, imagine you are a person of color. You may find that the algorithm predicted you to be much more dangerous in the future than a white individual who did something similar or worse, and you receive a harsher sentence. Also, imagine you learn that information about your previous offenses and personal history was fed to the algorithm without your consent. The algorithm was designed to determine how to rehabilitate people, not how to sentence them.

This situation often happens with the use of a predictive algorithm called COMPAS, which analyzes answers to 137 questions, ranging from criminal history to personal background, such as whether your parents were incarcerated or whether you have a job, to assess your risk level. The tool is designed to forecast the likelihood of a criminal defendant reoffending and is used at several stages of the U.S. criminal justice system, including pretrial release decisions and sentencing. COMPAS generates risk scores by analyzing data collected from interviews and criminal records.

Julia Angwin and her colleagues at ProPublica conducted an investigation into COMPAS using data they obtained from the Broward County Sheriff’s Office through the Freedom of Information Act. They collected real-world COMPAS risk scores for over 7,000 individuals arrested in Broward County, Florida, and compared them with actual criminal records over the following two years. This data was used to statistically evaluate the accuracy and fairness of COMPAS predictions. Although the algorithm was about 61% accurate overall, slightly better than random guessing, Angwin’s team found that it made systematic errors based on race. The scores often overestimated risk for Black defendants and underestimated it for white defendants.

Angwin’s analysis raises serious ethical concerns. Black defendants were far more likely to be labeled high-risk even when they did not reoffend, while white defendants with worse records were often labeled low-risk. These scores can and have influenced people’s freedom, yet the individuals affected do not know how the scores are calculated and have limited ability to challenge them. This lack of transparency and accountability raises urgent concerns about fairness and racial bias in automated decision-making systems used by the justice system.

What was the consent structure for recruiting participants? Were the participants aware of the ways their data would be used for research? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen?

The core issue here is informed consent, which requires that participants understand how their data will be used and voluntarily agree to those uses. In the case of COMPAS, individuals were not recruited as research participants. They were criminal defendants whose data was collected to determine the proceedings of their court cases. As Angwin’s investigation shows, defendants were likely unaware that their answers to personal questions, such as whether a parent had been to prison, would later be used in both court proceedings and in external journalism and academic research. Issie Lapowsky, in her WIRED article, reinforces this concern by explaining how data originally collected for case processing was later used to evaluate algorithmic fairness and predictive accuracy, often without any informed consent from the individuals involved. This raises serious questions about whether meaningful consent is even possible in a setting like the criminal justice system. Both Angwin and Lapowsky show how difficult it is to secure valid consent for future and unforeseen uses, especially when those uses include third-party audits, journalistic investigations, or machine learning research.

Is the data being used in unintended ways to the original study?

The main issue here is mitigating bias in both our models and ourselves, especially when we apply predictive tools to situations that directly affect people’s freedom. This involves identifying and correcting disparities in how predictive models treat different demographic groups. Angwin’s investigation showed that COMPAS produced racially skewed predictions. Black defendants were far more likely than white defendants to be falsely labeled as high-risk. Although COMPAS does not include race as a direct input, the model incorporates proxy variables, such as education level, employment status, and family history, that are deeply shaped by systemic racial inequalities. Lapowsky expands on this point by describing how Dartmouth researchers compared COMPAS to predictions made by Mechanical Turk workers and found similar racial disparities, even though neither group used race as a variable. This suggests that bias is embedded in the structure of the data itself. The fact that COMPAS was used in sentencing decisions, even though it was originally intended for case management, shows how predictive tools can be used in unintended ways, increasing the risk of harm.

Should race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender?

The core question here is whether including or excluding race makes a model more fair or more biased. According to Angwin, COMPAS does not directly include race as a variable. However, variables like prior convictions, family incarceration, and employment status serve as proxies. These are not neutral features; they are shaped by long-standing inequalities in housing, education, and policing. Lapowsky’s article confirms that even when race was withheld from both humans and algorithms, false positive rates were still higher for Black defendants. This shows that excluding race does not eliminate bias but instead makes it harder to detect and correct. Dartmouth researchers found that just two variables—age and number of prior offenses—were enough to replicate COMPAS’s predictions, but these also reflect racial disparities in policing and prosecution. Gender works in a similar way. While it may appear to be a neutral input, it can reinforce stereotypes and institutional biases if not handled carefully. Both Angwin and Lapowsky argue that removing sensitive attributes like race or gender from predictive models does not ensure fairness and may actually hide inequities. One possibility would be to explicitly include race and use it to weigh certain variables, such as family incarceration and employment status, less heavily for historically marginalized groups. However, this strategy could backfire and introduce other forms of unfairness if not applied carefully.

How were the variables collected? Were they accurately recorded? Is there any missing data?

The key concern here is data integrity. In Angwin’s investigation, COMPAS relied on both self-reported questionnaires and criminal records. Many of the 137 variables are based on personal opinions or unverifiable information, such as a person’s views on authority or their peers’ behavior. This raises questions about accuracy, especially in institutional settings where individuals may not respond freely or truthfully. Lapowsky also notes that COMPAS collects dozens of data points per defendant, many of which are opaque or not publicly disclosed. The company behind COMPAS has never published the full list of variables or how they are weighted, making it impossible to audit the system. This lack of transparency means researchers and affected individuals cannot check for missing data or inconsistent methods. According to Lapowsky, even when researchers used only seven variables in a simplified model, they achieved similar accuracy to COMPAS. This suggests that much of the additional data may not improve performance and could introduce more bias or error.

Citations:

Angwin, Julia, et al. "Machine Bias: There’s Software Used across the Country to Predict Future Criminals. And It’s Biased against Blacks." ProPublica, 23 May 2016, www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.

Lapowsky, Issie. "Crime-Predicting Algorithms May Not Fare Much Better Than Untrained Humans." WIRED, 17 Jan. 2018, https://www.wired.com/story/crime-predicting-algorithms-may-not-outperform-untrained-humans/.
